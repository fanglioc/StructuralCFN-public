
from typing import Dict, Optional, Tuple, Type

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F


class FunctionNode(nn.Module):
    """
    Base class for all function nodes in the compositional network.

    Each function node represents a mathematical operation.

    Attributes:
        input_dim (int): The dimensionality of the input.
        output_dim (int): The dimensionality of the output.
    """

    def __init__(self, input_dim: int):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim: Optional[int] = None  # Must be set by subclasses

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Defines the forward pass of the function node.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor.
        """
        raise NotImplementedError("Subclasses must implement the forward method")

    def describe(self) -> str:
        """
        Returns a string description of the function node and its parameters.

        Returns:
            str: The description of the function node.
        """
        params = {
            name: f"{param.detach().cpu().numpy().round(3)}"
            for name, param in self.named_parameters()
        }
        return f"{self.__class__.__name__} | Formula: {self.formula()} | params: {params}"

    def formula(self) -> str:
        """Returns a symbolic mathematical string for this node."""
        return "f(x)"

    def set_trainable(self, trainable: bool):
        """
        Sets the `requires_grad` attribute of the node's parameters.

        Args:
            trainable (bool): Whether the parameters should be trainable.
        """
        for param in self.parameters():
            param.requires_grad = trainable


# region --- Standard Function Nodes ---


class LinearFunctionNode(FunctionNode):
    """
    A linear function node: `f(x) = xW + b`.

    Args:
        input_dim (int): The dimensionality of the input.
        output_dim (int): The dimensionality of the output. Defaults to 1.
        weights (Optional[torch.Tensor]): The initial weights. If None,
            Kaiming uniform initialization is used. Defaults to None.
        bias (Optional[torch.Tensor]): The initial bias. If None, initialized
            to zeros. Defaults to None.
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int = 1,
        weights: Optional[torch.Tensor] = None,
        bias: Optional[torch.Tensor] = None,
    ):
        super().__init__(input_dim)
        self.output_dim = output_dim

        if weights is None:
            scale = np.sqrt(2.0 / input_dim) if input_dim > 0 else 1.0
            weights = torch.randn(input_dim, output_dim) * scale
        self.weights = nn.Parameter(torch.as_tensor(weights, dtype=torch.float32))

        if bias is None:
            bias = torch.zeros(output_dim)
        self.bias = nn.Parameter(torch.as_tensor(bias, dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.matmul(x, self.weights) + self.bias


class ReLUFunctionNode(FunctionNode):
    """
    A Rectified Linear Unit (ReLU) function node: `f(x) = max(0, x)`.

    Args:
        input_dim (int): The dimensionality of the input.
    """

    def __init__(self, input_dim: int):
        super().__init__(input_dim)
        self.output_dim = input_dim
        self.relu = nn.ReLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.relu(x)


class SinusoidalFunctionNode(FunctionNode):
    """
    A sinusoidal function node: `f(x) = amplitude * sin(frequency * (x·direction) + phase)`.

    Args:
        input_dim (int): The dimensionality of the input.
        frequency (float): The frequency of the sinusoid. Defaults to 1.0.
        amplitude (float): The amplitude of the sinusoid. Defaults to 1.0.
        phase (float): The phase of the sinusoid. Defaults to 0.0.
        direction (Optional[torch.Tensor]): The direction vector. If None,
            initialized randomly. Defaults to None.
    """

    def __init__(
        self,
        input_dim: int,
        frequency: float = 1.0,
        amplitude: float = 1.0,
        phase: float = 0.0,
        direction: Optional[torch.Tensor] = None,
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        if direction is None:
            direction = torch.randn(input_dim)
            direction /= torch.norm(direction) if torch.norm(direction) > 0 else 1.0
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
        self.frequency = nn.Parameter(torch.tensor(float(frequency), dtype=torch.float32))
        self.amplitude = nn.Parameter(torch.tensor(float(amplitude), dtype=torch.float32))
        self.phase = nn.Parameter(torch.tensor(float(phase), dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        frequency = torch.clamp(self.frequency, min=1e-6)
        amplitude = torch.clamp(self.amplitude, min=1e-6)
        projection = torch.matmul(x, self.direction)
        return (amplitude * torch.sin(frequency * projection + self.phase)).unsqueeze(-1)

    def formula(self) -> str:
        a = self.amplitude.item()
        f = self.frequency.item()
        p = self.phase.item()
        return f"{a:.3f} * sin({f:.3f} * u + {p:.3f})"


class PolynomialFunctionNode(FunctionNode):
    """
    A polynomial function node: `f(x) = c_0 + c_1*(x·d) + c_2*(x·d)^2 + ...`

    Args:
        input_dim (int): The dimensionality of the input.
        degree (int): The degree of the polynomial. Defaults to 3.
        coefficients (Optional[torch.Tensor]): The polynomial coefficients.
            If None, initialized randomly. Defaults to None.
        direction (Optional[torch.Tensor]): The direction vector. If None,
            initialized randomly. Defaults to None.
    """

    def __init__(
        self,
        input_dim: int,
        degree: int = 3,
        coefficients: Optional[torch.Tensor] = None,
        direction: Optional[torch.Tensor] = None,
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        self.degree = degree
        if direction is None:
            direction = torch.randn(input_dim)
            direction /= torch.norm(direction) if torch.norm(direction) > 0 else 1.0
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
        if coefficients is None:
            coefficients = torch.randn(degree + 1) / torch.arange(
                1, degree + 2, dtype=torch.float32
            )
        self.coefficients = nn.Parameter(
            torch.as_tensor(coefficients, dtype=torch.float32)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        projection = torch.matmul(x, self.direction)
        powers = torch.stack([projection**i for i in range(self.degree + 1)], dim=1)
        return torch.matmul(powers, self.coefficients).unsqueeze(-1)

    def formula(self) -> str:
        coeffs = self.coefficients.detach().cpu().numpy()
        terms = []
        for i, c in enumerate(coeffs):
            if abs(c) < 1e-4: continue
            if i == 0: terms.append(f"{c:.3f}")
            elif i == 1: terms.append(f"{c:.3f}*u")
            else: terms.append(f"{c:.3f}*u^{i}")
        return " + ".join(terms).replace("+ -", "- ")


class GaussianFunctionNode(FunctionNode):
    """
    A Gaussian function node: `f(x) = exp(-||x - center||^2 / (2 * width^2))`.

    Args:
        input_dim (int): The dimensionality of the input.
        center (Optional[torch.Tensor]): The center of the Gaussian. If None,
            initialized randomly. Defaults to None.
        width (float): The width of the Gaussian. Defaults to 1.0.
    """

    def __init__(
        self, input_dim: int, center: Optional[torch.Tensor] = None, width: float = 1.0
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        if center is None:
            center = torch.randn(input_dim)
        self.center = nn.Parameter(torch.as_tensor(center, dtype=torch.float32))
        self.width = nn.Parameter(torch.tensor(float(width), dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        width = torch.clamp(self.width, min=1e-6)
        squared_dist = torch.sum((x - self.center) ** 2, dim=1, keepdim=True)
        return torch.exp(-squared_dist / (2 * width**2))


class ProjectedGaussianFunctionNode(FunctionNode):
    """
    A Projected Gaussian function node: `f(x) = exp(-(x·d - center)^2 / (2 * width^2))`.

    Captures "localized spikes" or "normal ranges" along a specific projection direction.
    """

    def __init__(
        self,
        input_dim: int,
        direction: Optional[torch.Tensor] = None,
        center: float = 0.0,
        width: float = 1.0,
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        if direction is None:
            direction = torch.randn(input_dim)
            direction /= torch.norm(direction) if torch.norm(direction) > 0 else 1.0
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
        self.center = nn.Parameter(torch.tensor(float(center), dtype=torch.float32))
        self.width = nn.Parameter(torch.tensor(float(width), dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        width = torch.clamp(self.width, min=1e-6)
        projection = torch.matmul(x, self.direction)
        diff = projection - self.center
        return torch.exp(-(diff**2) / (2 * width**2)).unsqueeze(-1)


class RationalFunctionNode(FunctionNode):
    """
    A Rational function node (Padé Approximant): `f(x) = P(x·d) / (1 + |Q(x·d)|)`.

    Can model asymptotes, saturation, and sharp regime changes.
    """

    def __init__(
        self,
        input_dim: int,
        direction: Optional[torch.Tensor] = None,
        num_degree: int = 2,
        den_degree: int = 2,
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        self.num_degree = num_degree
        self.den_degree = den_degree

        if direction is None:
            direction = torch.randn(input_dim)
            direction /= torch.norm(direction) if torch.norm(direction) > 0 else 1.0
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))

        # Numerator coefficients (a_0 + a_1*z + a_2*z^2 + ...)
        self.a = nn.Parameter(torch.randn(num_degree + 1) * 0.1)
        # Denominator coefficients (b_1*z + b_2*z^2 + ...)
        # Constant 1 is added in forward pass for stability
        self.b = nn.Parameter(torch.randn(den_degree) * 0.1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = torch.matmul(x, self.direction)

        # Numerator
        num = self.a[0]
        for i in range(1, self.num_degree + 1):
            num = num + self.a[i] * (z**i)

        # Denominator: 1 + |sum(b_j * z^j)| to ensure no division by zero
        den_sum = 0
        for j in range(1, self.den_degree + 1):
            den_sum = den_sum + self.b[j - 1] * (z**j)

        den = 1.0 + torch.abs(den_sum)
        return (num / den).unsqueeze(-1)


class SigmoidFunctionNode(FunctionNode):
    """
    A sigmoid function node.

    - If `direction` is provided, it computes: `f(x) = 1 / (1 + exp(-s * (x·d + o)))`
    - If `direction` is None, it computes element-wise: `f(x) = 1 / (1 + exp(-s * (x + o)))`

    Args:
        input_dim (int): The dimensionality of the input.
        direction (Optional[torch.Tensor]): The direction vector. If None,
            the sigmoid is applied element-wise. Defaults to None.
        offset (float): The offset. Defaults to 0.0.
        steepness (float): The steepness of the sigmoid. Defaults to 1.0.
    """

    def __init__(
        self,
        input_dim: int,
        direction: Optional[torch.Tensor] = None,
        offset: float = 0.0,
        steepness: float = 1.0,
    ):
        super().__init__(input_dim)

        self.is_elementwise = direction is None

        if self.is_elementwise:
            self.output_dim = input_dim
            self.offset = nn.Parameter(
                torch.full((input_dim,), float(offset), dtype=torch.float32)
            )
            self.steepness = nn.Parameter(
                torch.full((input_dim,), float(steepness), dtype=torch.float32)
            )
        else:
            self.output_dim = 1
            if direction is None:
                direction = torch.randn(input_dim)
                direction = direction / torch.norm(direction)
            self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
            self.offset = nn.Parameter(torch.tensor(float(offset), dtype=torch.float32))
            self.steepness = nn.Parameter(
                torch.tensor(float(steepness), dtype=torch.float32)
            )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.is_elementwise:
            return torch.sigmoid(self.steepness * (x + self.offset))
        else:
            projection = torch.matmul(x, self.direction) + self.offset
            return torch.sigmoid(self.steepness * projection).unsqueeze(-1)

    def formula(self) -> str:
        if self.is_elementwise:
            return "σ(s * (x + o))"
        s = self.steepness.item()
        o = self.offset.item()
        return f"σ({s:.3f} * (u + {o:.3f}))"


class StepFunctionNode(FunctionNode):
    """
    A smoothed, differentiable step function.

    `f(x) = 1 / (1 + exp(-smoothing * (x·d - bias)))`

    Args:
        input_dim (int): The dimensionality of the input.
        direction (torch.Tensor): The direction vector.
        bias (float): The bias (threshold) of the step.
        smoothing (float): The smoothing factor. Defaults to 10.0.
    """

    def __init__(
        self, input_dim: int, direction: torch.Tensor, bias: float, smoothing: float = 10.0
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
        self.bias = nn.Parameter(torch.tensor(float(bias), dtype=torch.float32))
        self.smoothing = nn.Parameter(torch.tensor(float(smoothing), dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        projection = torch.matmul(x, self.direction)
        return torch.sigmoid(self.smoothing * (projection - self.bias)).unsqueeze(-1)


class ExponentialFunctionNode(FunctionNode):
    """
    An exponential function node.

    `f(x) = scale * exp(rate * (x·d - shift))`

    Args:
        input_dim (int): The dimensionality of the input.
        direction (torch.Tensor): The direction vector.
        rate (float): The rate of the exponential.
        shift (float): The shift of the exponential.
        scale (float): The scale of the exponential.
    """

    def __init__(
        self,
        input_dim: int,
        direction: torch.Tensor,
        rate: float,
        shift: float,
        scale: float,
    ):
        super().__init__(input_dim)
        self.output_dim = 1
        self.direction = nn.Parameter(torch.as_tensor(direction, dtype=torch.float32))
        self.rate = nn.Parameter(torch.tensor(float(rate), dtype=torch.float32))
        self.shift = nn.Parameter(torch.tensor(float(shift), dtype=torch.float32))
        self.scale = nn.Parameter(torch.tensor(float(scale), dtype=torch.float32))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        projection = torch.matmul(x, self.direction)
        return (self.scale * torch.exp(self.rate * (projection - self.shift))).unsqueeze(
            -1
        )


# endregion


# region --- Image-Specific Function Nodes ---


class FourierFunctionNode(FunctionNode):
    """
    A function node that computes Fourier features from an image.

    Args:
        input_dim (int): The dimensionality of the flattened input image.
        image_size (Tuple[int, int]): The (height, width) of the image.
            Defaults to (32, 32).
        n_channels (int): The number of channels in the image. Defaults to 3.
        n_features (int): The number of Fourier features to compute.
            Defaults to 1.
    """

    def __init__(
        self,
        input_dim: int,
        image_size: Tuple[int, int] = (32, 32),
        n_channels: int = 3,
        n_features: int = 1,
    ):
        super().__init__(input_dim)
        assert input_dim == image_size[0] * image_size[1] * n_channels
        self.output_dim = n_features * 2
        self.image_size = image_size
        self.n_channels = n_channels
        self.n_features = n_features
        self.freqs = nn.Parameter(torch.randn(n_features, 2))
        self.phases = nn.Parameter(torch.randn(n_features))
        self.channel_weights = nn.Parameter(torch.randn(n_features, n_channels))
        grid_x, grid_y = torch.meshgrid(
            torch.linspace(0, 1, image_size[0]),
            torch.linspace(0, 1, image_size[1]),
            indexing="ij",
        )
        self.register_buffer("grid", torch.stack([grid_x, grid_y], dim=-1))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        x_img = x.view(
            batch_size, self.n_channels, self.image_size[0], self.image_size[1]
        )
        basis_arg = (
            2
            * np.pi
            *
            (
                self.grid[..., 0] * self.freqs[:, 0].view(-1, 1, 1)
                + self.grid[..., 1] * self.freqs[:, 1].view(-1, 1, 1)
            )
            + self.phases.view(-1, 1, 1)
        )
        cos_basis = torch.cos(basis_arg)
        sin_basis = torch.sin(basis_arg)
        cos_weighted_channels = torch.einsum("bchw,fhw->bfc", x_img, cos_basis)
        sin_weighted_channels = torch.einsum("bchw,fhw->bfc", x_img, sin_basis)
        cos_output = torch.einsum(
            "bfc,fc->bf", cos_weighted_channels, self.channel_weights
        )
        sin_output = torch.einsum(
            "bfc,fc->bf", sin_weighted_channels, self.channel_weights
        )
        return torch.cat([cos_output, sin_output], dim=1)


class GaborFunctionNode(FunctionNode):
    """
    A function node that applies a Gabor filter to an image.

    Args:
        input_dim (int): The dimensionality of the flattened input image.
        image_size (Tuple[int, int]): The (height, width) of the image.
            Defaults to (32, 32).
        n_channels (int): The number of channels in the image. Defaults to 3.
    """

    def __init__(
        self, input_dim: int, image_size: Tuple[int, int] = (32, 32), n_channels: int = 3
    ):
        super().__init__(input_dim)
        assert input_dim == image_size[0] * image_size[1] * n_channels
        self.output_dim = 1
        self.image_size = image_size
        self.n_channels = n_channels
        self.frequency = nn.Parameter(torch.randn(1))
        self.theta = nn.Parameter(torch.randn(1))
        self.sigma_x = nn.Parameter(torch.randn(1))
        self.sigma_y = nn.Parameter(torch.randn(1))
        self.psi = nn.Parameter(torch.randn(1))
        self.channel_weights = nn.Parameter(torch.randn(n_channels))
        grid_x, grid_y = torch.meshgrid(
            torch.linspace(-image_size[0] // 2, image_size[0] // 2, image_size[0]),
            torch.linspace(-image_size[1] // 2, image_size[1] // 2, image_size[1]),
            indexing="ij",
        )
        self.register_buffer("grid", torch.stack([grid_x, grid_y], dim=-1))

    def _get_gabor_kernel(self) -> torch.Tensor:
        sigma_x = torch.clamp(self.sigma_x, min=1e-6)
        sigma_y = torch.clamp(self.sigma_y, min=1e-6)
        x_prime = (
            self.grid[..., 0] * torch.cos(self.theta)
            + self.grid[..., 1] * torch.sin(self.theta)
        )
        y_prime = (
            -self.grid[..., 0] * torch.sin(self.theta)
            + self.grid[..., 1] * torch.cos(self.theta)
        )
        envelope = torch.exp(-0.5 * (x_prime**2 / sigma_x**2 + y_prime**2 / sigma_y**2))
        carrier = torch.cos(2 * np.pi * self.frequency * x_prime + self.psi)
        return envelope * carrier

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        x_img = x.view(
            batch_size, self.n_channels, self.image_size[0], self.image_size[1]
        )
        gabor_kernel = self._get_gabor_kernel()
        response = torch.einsum("bchw,hw->bc", x_img, gabor_kernel)
        output = torch.einsum("bc,c->b", response, self.channel_weights).unsqueeze(-1)
        return output


# endregion


# region --- DeepCFN Nodes ---


class GenericConvNode(FunctionNode):
    """
    A generic 2D convolutional node.

    Args:
        input_shape (Tuple[int, int]): The (height, width) of the input.
        in_channels (int): The number of input channels.
        out_channels (int): The number of output channels.
        kernel_size (int): The size of the convolutional kernel.
        padding (int): The padding to apply. Defaults to 0.
        stride (int): The stride of the convolution. Defaults to 1.
    """

    def __init__(
        self,
        input_shape: Tuple[int, int],
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        padding: int = 0,
        stride: int = 1,
    ):
        self.input_shape_2d = (in_channels, input_shape[0], input_shape[1])
        input_dim = in_channels * input_shape[0] * input_shape[1]
        super().__init__(input_dim)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.padding = padding
        self.stride = stride
        self.weight = nn.Parameter(
            torch.randn(out_channels, in_channels, kernel_size, kernel_size)
        )
        self.bias = nn.Parameter(torch.randn(out_channels))
        nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))
        output_h = (input_shape[0] + 2 * padding - kernel_size) // stride + 1
        output_w = (input_shape[1] + 2 * padding - kernel_size) // stride + 1
        self.output_shape_2d = (out_channels, output_h, output_w)
        self.output_dim = out_channels * output_h * output_w

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        x_2d = x.view(
            batch_size,
            self.in_channels,
            self.input_shape_2d[1],
            self.input_shape_2d[2],
        )
        conv_out = nn.functional.conv2d(
            x_2d, self.weight, self.bias, self.stride, self.padding
        )
        return conv_out.view(batch_size, -1)


class PoolingNode(FunctionNode):
    """
    An average pooling node.

    Args:
        input_shape (Tuple[int, int]): The (height, width) of the input.
        in_channels (int): The number of input channels.
        kernel_size (int): The size of the pooling kernel. Defaults to 2.
        stride (int): The stride of the pooling. Defaults to 2.
    """

    def __init__(
        self,
        input_shape: Tuple[int, int],
        in_channels: int,
        kernel_size: int = 2,
        stride: int = 2,
    ):
        self.input_shape_2d = (in_channels, input_shape[0], input_shape[1])
        input_dim = in_channels * input_shape[0] * input_shape[1]
        super().__init__(input_dim)
        self.in_channels = in_channels
        self.kernel_size = kernel_size
        self.stride = stride
        output_h = (input_shape[0] - kernel_size) // stride + 1
        output_w = (input_shape[1] - kernel_size) // stride + 1
        self.output_shape_2d = (in_channels, output_h, output_w)
        self.output_dim = in_channels * output_h * output_w

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]
        x_2d = x.view(
            batch_size,
            self.in_channels,
            self.input_shape_2d[1],
            self.input_shape_2d[2],
        )
        pool_out = nn.functional.avg_pool2d(x_2d, self.kernel_size, self.stride)
        return pool_out.view(batch_size, -1)


# endregion


# region --- Shared Patch Node ---


class SharedPatchFunctionNode(FunctionNode):
    """
    Applies a basic FunctionNode to patches of an image with shared parameters.

    Args:
        input_image_shape (Tuple[int, int, int]): The (channels, height, width)
            of the input image.
        patch_size (Tuple[int, int]): The (height, width) of the patches.
        stride (Tuple[int, int]): The (height, width) of the stride.
        sub_node_type (str): The name of the function node to apply to each patch.
        sub_node_kwargs (Dict): The keyword arguments for the sub-node.
        combination (str): How to combine the outputs from the patches.
            One of 'concat', 'mean', or 'max'. Defaults to 'concat'.
    """

    def __init__(
        self,
        input_image_shape: Tuple[int, int, int],
        patch_size: Tuple[int, int],
        stride: Tuple[int, int],
        sub_node_type: str,
        sub_node_kwargs: Dict,
        combination: str = "concat",
    ):
        # Calculate the flattened input dimension for the full image
        input_dim = input_image_shape[0] * input_image_shape[1] * input_image_shape[2]
        super().__init__(input_dim)

        self.input_image_shape = input_image_shape
        self.patch_size = patch_size
        self.stride = stride
        self.combination = combination

        # Calculate input dimension for a single patch
        patch_input_dim = patch_size[0] * patch_size[1] * input_image_shape[0]

        # Create the sub-node that will operate on each patch
        self.sub_node = FunctionNodeFactory.create(
            sub_node_type, input_dim=patch_input_dim, **sub_node_kwargs
        )

        # Calculate output dimensions
        # Calculate number of patches
        output_h = (input_image_shape[1] - patch_size[0]) // stride[0] + 1
        output_w = (input_image_shape[2] - patch_size[1]) // stride[1] + 1
        self.num_patches = output_h * output_w

        if self.combination == "concat":
            self.output_dim = self.num_patches * self.sub_node.output_dim
        elif self.combination in ["mean", "max"]:
            # If combining with mean/max, the sub_node must output a single value per patch
            if self.sub_node.output_dim != 1:
                raise ValueError(
                    f"Sub-node must have output_dim=1 for '{combination}' combination."
                )
            self.output_dim = 1  # Single global feature
        else:
            raise ValueError(f"Unknown combination method: {combination}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size = x.shape[0]

        # Reshape flattened input to image format (channels, height, width)
        x_img = x.view(
            batch_size,
            self.input_image_shape[0],
            self.input_image_shape[1],
            self.input_image_shape[2],
        )

        # Extract patches using unfold
        # Output shape: (batch_size, C*P_h*P_w, num_patches)
        patches = F.unfold(x_img, kernel_size=self.patch_size, stride=self.stride)

        # Reshape patches for the sub_node: (batch_size * num_patches, patch_input_dim)
        patches_reshaped = patches.transpose(1, 2).reshape(-1, patches.shape[1])

        # Apply the shared sub_node to all patches
        sub_node_outputs = self.sub_node(patches_reshaped)

        # Reshape back to (batch_size, num_patches, sub_node_output_dim)
        sub_node_outputs = sub_node_outputs.view(
            batch_size, self.num_patches, self.sub_node.output_dim
        )

        # Combine patch outputs
        if self.combination == "concat":
            return sub_node_outputs.view(batch_size, -1)  # Flatten all patch outputs
        elif self.combination == "mean":
            return torch.mean(sub_node_outputs, dim=1)  # Average across patches
        elif self.combination == "max":
            return torch.max(sub_node_outputs, dim=1)[0]  # Max across patches

    def describe(self) -> str:
        return (
            f"{self.__class__.__name__}(input_shape={self.input_image_shape}, "
            f"patch_size={self.patch_size}, stride={self.stride}, "
            f"sub_node={self.sub_node.describe()}, combination='{self.combination}')"
        )


# endregion


# region --- Regularization Nodes ---


class DropoutFunctionNode(FunctionNode):
    """
    A dropout function node.

    Args:
        input_dim (int): The dimensionality of the input.
        p (float): The probability of an element to be zeroed. Defaults to 0.5.
    """

    def __init__(self, input_dim: int, p: float = 0.5):
        super().__init__(input_dim)
        self.output_dim = input_dim  # Dropout doesn't change dimension
        self.dropout = nn.Dropout(p=p)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.dropout(x)


# endregion


# region --- Wrapper Nodes ---


class SequentialWrapperNode(FunctionNode):
    """
    A node that wraps a sequence of function nodes.

    Args:
        function_nodes (list[FunctionNode]): The list of function nodes to wrap.
        input_dim (Optional[int]): The input dimension. If provided, it must
            match the input dimension of the first node. Defaults to None.
    """

    def __init__(self, function_nodes, input_dim: Optional[int] = None):
        # Added input_dim parameter
        # Validate that the provided input_dim (if any) matches the first node's input_dim
        if input_dim is not None and input_dim != function_nodes[0].input_dim:
            raise ValueError(
                "Provided input_dim does not match the input_dim of the first function node."
            )
        super().__init__(function_nodes[0].input_dim)
        self.function_nodes = nn.ModuleList(function_nodes)

        # Validate dimensions and set output_dim
        for i in range(1, len(function_nodes)):
            if function_nodes[i].input_dim != function_nodes[i - 1].output_dim:
                raise ValueError(
                    f"Dimension mismatch at index {i} in SequentialWrapperNode"
                )

        self.output_dim = function_nodes[-1].output_dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for node in self.function_nodes:
            x = node(x)
        return x

    def describe(self) -> str:
        lines = [f"{self.__class__.__name__} (Sequential):"]
        for i, node in enumerate(self.function_nodes):
            lines.append(f"  - Step {i + 1}: {node.describe()}")
        return "\n".join(lines)


class IdentityFunctionNode(FunctionNode):
    """
    An identity function node: `f(x) = x`.

    Args:
        input_dim (int): The dimensionality of the input.
    """

    def __init__(self, input_dim: int):
        super().__init__(input_dim)
        self.output_dim = input_dim  # Output dimension is same as input dimension

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x

    def describe(self) -> str:
        return f"{self.__class__.__name__} with params: {{}}"


class ResidualWrapperNode(FunctionNode):
    """
    A node that adds a residual connection around a main path.

    `f(x) = main_path(x) + shortcut(x)`

    Args:
        input_dim (int): The dimensionality of the input.
        main_path_nodes (list[FunctionNode]): The list of function nodes in the
            main path.
    """

    def __init__(self, input_dim: int, main_path_nodes):
        super().__init__(input_dim)
        self.main_path = SequentialWrapperNode(main_path_nodes, input_dim=input_dim)

        # If dimensions don't match, add a linear projection to the shortcut
        if self.main_path.output_dim != input_dim:
            self.shortcut = FunctionNodeFactory.create(
                "Linear", input_dim=input_dim, output_dim=self.main_path.output_dim
            )
        else:
            self.shortcut = FunctionNodeFactory.create("Identity", input_dim=input_dim)

        self.output_dim = self.main_path.output_dim

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.main_path(x) + self.shortcut(x)

    def describe(self) -> str:
        return f"{self.__class__.__name__} (Main Path: {self.main_path.describe()}, Shortcut: {self.shortcut.describe()})"


class AttentionFunctionNode(FunctionNode):
    """
    A mathematical function node that computes attention as a weighted sum operation.

    The attention operation is defined as:
    1. Compute similarity between elements: sim(x_i, x_j) = (x_i·x_j)/sqrt(d)
    2. Normalize similarities with softmax: w_ij = exp(sim_ij)/sum_k(exp(sim_ik))
    3. Compute weighted sum: y_i = sum_j(w_ij * x_j)

    Args:
        input_dim (int): The dimensionality of the input.
        scale_factor (Optional[float]): The scale factor for the dot product.
            If None, defaults to `1 / sqrt(input_dim)`. Defaults to None.
    """

    def __init__(self, input_dim: int, scale_factor: Optional[float] = None):
        super().__init__(input_dim)
        self.input_dim = input_dim
        # Scale factor for dot product (typically 1/sqrt(dimension))
        self.scale_factor = scale_factor or (input_dim**-0.5)
        self.output_dim = input_dim

    def forward(
        self, x: torch.Tensor, context_shape: Optional[Tuple[int, int]] = None
    ) -> torch.Tensor:
        """
        Apply attention as a mathematical operation.

        Args:
            x (torch.Tensor): The input tensor of shape
                `[batch_size * num_patches, input_dim]`.
            context_shape (Optional[Tuple[int, int]]): The (batch_size, num_patches)
                to reshape the input. If None, it is inferred. Defaults to None.

        Returns:
            torch.Tensor: The output tensor.
        """
        if context_shape is None:
            # Default reshape assumes x is [batch_size * num_patches, input_dim]
            batch_size = x.shape[0] // 225  # Default to 225 patches
            num_patches = 225
        else:
            batch_size, num_patches = context_shape

        # Reshape to [batch_size, num_patches, input_dim]
        x_reshaped = x.view(batch_size, num_patches, self.input_dim)

        # Compute similarity matrix - mathematical dot product operation
        # [batch_size, num_patches, num_patches]
        similarity = torch.bmm(x_reshaped, x_reshaped.transpose(1, 2)) * self.scale_factor

        # Apply softmax to get normalized weights - mathematical exponential and normalization
        attention_weights = F.softmax(similarity, dim=-1)

        # Apply attention weights - mathematical weighted sum
        attended_features = torch.bmm(attention_weights, x_reshaped)

        # Reshape back to original format
        return attended_features.view(batch_size * num_patches, self.input_dim)

    def describe(self) -> str:
        return f"AttentionFunctionNode: Mathematical attention operation with scale={self.scale_factor}"


# endregion


class FunctionNodeFactory:
    """
    A factory for creating function nodes.
    """

    _node_types: Dict[str, Type[FunctionNode]] = {
        "Linear": LinearFunctionNode,
        "ReLU": ReLUFunctionNode,
        "Sinusoidal": SinusoidalFunctionNode,
        "Polynomial": PolynomialFunctionNode,
        "Gaussian": GaussianFunctionNode,
        "Sigmoid": SigmoidFunctionNode,
        "Fourier": FourierFunctionNode,
        "Gabor": GaborFunctionNode,
        "Step": StepFunctionNode,
        "Exponential": ExponentialFunctionNode,
        "ProjectedGaussian": ProjectedGaussianFunctionNode,
        "Rational": RationalFunctionNode,
        "GenericConv": GenericConvNode,
        "Pooling": PoolingNode,
        "SharedPatch": SharedPatchFunctionNode,
        "Dropout": DropoutFunctionNode,
        "SequentialWrapper": SequentialWrapperNode,
        "Identity": IdentityFunctionNode,
        "ResidualWrapper": ResidualWrapperNode,
        "MathematicalAttention": AttentionFunctionNode,
    }

    @classmethod
    def create(cls, node_type: str, **kwargs) -> FunctionNode:
        """
        Creates a function node of the specified type.

        Args:
            node_type (str): The type of the function node to create.
            **kwargs: The arguments for the function node's constructor.

        Returns:
            FunctionNode: The created function node.
        """
        if node_type not in cls._node_types:
            raise ValueError(f"Unknown function node type: {node_type}")
        return cls._node_types[node_type](**kwargs)

